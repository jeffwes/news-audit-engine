# News Audit Engine v4.0 - System Architecture

**Document Version**: 1.0  
**Last Updated**: December 24, 2025  
**Status**: Prototype - Fully Operational

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Overview](#system-overview)
3. [Architecture Layers](#architecture-layers)
4. [Data Flow](#data-flow)
5. [Component Details](#component-details)
6. [Dependencies & Infrastructure](#dependencies--infrastructure)
7. [Prompts & LLM Integration](#prompts--llm-integration)
8. [Configuration & Deployment](#configuration--deployment)
9. [API Specifications](#api-specifications)
10. [Future Enhancements](#future-enhancements)

---

## Executive Summary

The News Audit Engine is a sophisticated narrative integrity analysis system that moves beyond atomic fact-checking to evaluate the **logical coherence, evidence quality, and narrative stability** of news content. It employs a four-layer architecture combining Named Entity Recognition (NER), multi-intent web search, semantic conflict detection via vector databases, and multi-agent consensus protocols.

**Key Innovation**: Rather than verifying individual facts (brittle, context-dependent), the system audits the **structural integrity** of narrative pillars—the causal arguments that form a story's logical backbone.

**Design Philosophy**: Intellectual honesty over false precision. When evidence is contradictory, the system returns "Inconclusive" rather than a random guess.

---

## System Overview

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                         INPUT LAYER                              │
│  Article Text → CLI (cli.py) → Content Validation               │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                    LAYER 1: DECOMPOSITION                        │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐     │
│  │ Gemini API   │ →  │  NER Stage 1 │ →  │  NER Stage 2 │     │
│  │ Pillar       │    │ (Fast/md)    │    │ (Precise/trf)│     │
│  │ Extraction   │    │ Context Map  │    │ Entity Link  │     │
│  └──────────────┘    └──────────────┘    └──────────────┘     │
│         ↓                    ↓                     ↓            │
│    3-5 Pillars        Entity Context      Disambiguated         │
│                                           Entities + Dates       │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                   LAYER 2: DIVERSE SEARCH                        │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │              Tavily API (4 Search Strategies)             │  │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐   │  │
│  │  │Confirming│ │Adversarial│ │Contextual│ │Consensus │   │  │
│  │  │Evidence  │ │Debunking  │ │Background│ │Wires     │   │  │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘   │  │
│  └──────────────────────────────────────────────────────────┘  │
│         ↓                                                        │
│    ~180 Search Results per Article                              │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                LAYER 3: SEMANTIC JUDGE                           │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐     │
│  │ Gemini       │ →  │  Qdrant      │ →  │  Conflict    │     │
│  │ Embeddings   │    │  Vector DB   │    │  Detection   │     │
│  │ (768-dim)    │    │  In-Memory   │    │  Algorithm   │     │
│  └──────────────┘    └──────────────┘    └──────────────┘     │
│         ↓                    ↓                     ↓            │
│    Vector Space       Semantic Search      Opposing Evidence    │
│    Representation     (Cosine Similarity)  Identification       │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              LAYER 4: CONSENSUS PROTOCOL                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Multi-Agent Debate System                   │   │
│  │  ┌───────────┐  ┌────────────────┐  ┌──────────────┐  │   │
│  │  │The Auditor│  │The Contextualist│  │The Skeptic   │  │   │
│  │  │Logic &    │  │Temporal &       │  │Evidence      │  │   │
│  │  │Framing    │  │History          │  │Quality       │  │   │
│  │  └───────────┘  └────────────────┘  └──────────────┘  │   │
│  │       ↓                ↓                   ↓            │   │
│  │  Initial Verdict  →  2-Round Debate  →  Final Verdict  │   │
│  └─────────────────────────────────────────────────────────┘   │
│         ↓                                                        │
│    80% Consensus Threshold → Verdict or "Inconclusive"          │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                         OUTPUT LAYER                             │
│  Terminal Output + audit_result.json (Detailed Analysis)        │
└─────────────────────────────────────────────────────────────────┘
```

---

## Architecture Layers

### Layer 1: Decomposition (The Narrative & Entity Architect)

**Purpose**: Transform unstructured article text into a structured logical map of narrative pillars and disambiguated entities.

**Components**:

1. **Pillar Extraction** (Gemini LLM)
   - Input: Full article text (up to 8000 chars)
   - Output: 3-5 narrative pillars (causal arguments)
   - Model: `gemini-2.5-flash-lite`
   - Prompt: `prompts/pillar_extraction.json`

2. **Stage 1 NER: Contextual Grounding** (spaCy)
   - Model: `en_core_web_md` (50 MB, CPU-optimized)
   - Input: Full article text
   - Output: Entity map (PERSON, ORG, GPE, DATE)
   - Purpose: Build "knowledge map" to prevent LLM hallucination

3. **Stage 2 NER: Precision Extraction** (spaCy Transformer)
   - Model: `en_core_web_trf` (500 MB, transformer-based)
   - Input: Individual narrative pillars
   - Output: High-precision entities with:
     - Entity linking candidates (Wikidata IDs - TODO)
     - Temporal anchoring (absolute dates via `dateparser`)
     - Quote attribution (SaysWho integration - TODO)

**Key Innovation**: Two-stage approach balances speed (Stage 1) with precision (Stage 2), using the full article's context to disambiguate entities mentioned in individual pillars.

**File**: `src/ner_pipeline.py`

---

### Layer 2: Diverse Search (The Investigatory Team)

**Purpose**: Generate a comprehensive "Search Portfolio" representing diverse perspectives on each narrative pillar.

**Search Strategies**:

1. **Confirming Searches**
   - Query: `[Pillar claim] evidence`
   - Goal: Find supporting documentation
   - Example: "Harvard settlement talks evidence"

2. **Adversarial Searches**
   - Query: `[Pillar claim] debunked | false | criticism`
   - Goal: Find contradictory evidence
   - Example: "Trump Harvard negotiation debunked"

3. **Contextual Searches**
   - Query: `[Pillar claim] context | background | history`
   - Goal: Understand broader ecosystem
   - Example: "Trump administration university relations history"

4. **Consensus Searches**
   - Query: `site:reuters.com | site:apnews.com [Pillar claim]`
   - Goal: Establish authoritative baseline
   - Example: "site:reuters.com Harvard Trump dispute"

**API**: Tavily API (Advanced search depth)
- Returns: ~36 results per pillar (9 per strategy × 4 strategies)
- Total: ~180 results for 5-pillar article
- Content: LLM-optimized Markdown (ads/nav removed)

**File**: `src/search_portfolio.py`

---

### Layer 3: Synthesis & Conflict Detection (The Semantic Judge)

**Purpose**: Use vector similarity to detect semantic conflicts—situations where highly similar content represents opposing viewpoints.

**Vector Database**: Qdrant (in-memory mode)
- Configuration: 768 dimensions, Cosine distance
- Collection: `audit_evidence`
- Storage: Ephemeral (resets per analysis)

**Process**:

1. **Embedding Generation**
   - API: Gemini `text-embedding-004`
   - Input: Search result content (up to 500 chars)
   - Output: 768-dimensional vector
   - Metadata: pillar ID, search intent, source URL, credibility score

2. **Ingestion**
   - Batch upsert: ~180 vectors per article
   - Payload: Full metadata + truncated content

3. **Conflict Detection**
   - For each pillar:
     - Generate pillar embedding
     - Query Qdrant (top 20 results, >85% similarity)
     - Group by search intent
     - Flag if both `confirming` AND `adversarial` results present
   - Conflict type: `opposing_evidence`
   - Output: Supporting vs. Opposing source counts

4. **Conflict Classification** (NEW)
   - Purpose: Distinguish between different types of narrative conflicts to prevent false positives
   - Categories:
     * `factual_contradiction`: Mutually exclusive facts about the SAME event (undermines credibility)
     * `position_evolution`: Entity changed position over time (validates change stories)
     * `source_disagreement`: Competing claims about same timeframe (requires source evaluation)
   
   - Implementation:
     ```python
     def classify_conflict_type(self, pillar_text: str, conflict: Dict[str, Any]) -> Dict[str, str]:
         """Use LLM to classify conflict into one of three categories."""
         prompt = f"""Classify this narrative conflict.
         
         NARRATIVE CLAIM: {pillar_text}
         EVIDENCE CONFLICT: {conflict["supporting_count"]} supporting vs {conflict["opposing_count"]} opposing
         
         Classify as:
         A) factual_contradiction - mutually exclusive facts about SAME event
         B) position_evolution - entity changed position over time (both can be true)
         C) source_disagreement - competing claims about same timeframe
         
         CRITICAL: If claim describes NEW or CHANGED position, and opposing evidence shows OLD position,
         classify as B (position_evolution).
         
         Examples:
         - "Biden announces new policy X" + evidence of old policy Y → B (position_evolution)
         - "Biden signed bill yesterday" + evidence he did not → A (factual_contradiction)
         - "Experts say X" + other experts say Y → C (source_disagreement)
         """
         # Temperature 0.1 for consistent classification
         return self.gemini.generate_json(prompt, temperature=0.1)
     ```
   
   - Output: Each conflict includes `classification` and `classification_reasoning` fields
   - Usage: Layer 4 agents interpret conflicts differently based on classification
   - Known Limitation: Classification quality depends on pillar extraction capturing "change" language

4. **Omission Scoring** (TODO)
   - Compare consensus facts against article pillars
   - Calculate: `(missing_facts / total_consensus_facts)`
   - Flag when >50% omission rate

**File**: `src/semantic_judge.py`

---

### Layer 4: Consensus (The Stability Protocol)

**Purpose**: Multi-agent debate system to ensure verdicts are stable, explainable, and intellectually honest.

**Agent Personas** (from `prompts/agent_personas.json`):

1. **The Auditor**
   - Focus: Logical consistency, emotional manipulation, loaded language
   - Specialty: Identifying rhetorical fallacies and framing bias

2. **The Contextualist**
   - Focus: Temporal accuracy, historical continuity, causal logic
   - Specialty: Timeline coherence and event sequencing

3. **The Skeptic**
   - Focus: Source credibility, evidence sufficiency, methodological weaknesses
   - Specialty: Devil's advocate, challenges evidence quality

**Debate Protocol**:

1. **Initial Verdicts** (Turn 0)
   - Each agent independently analyzes evidence summary
   - Returns: `{verdict, confidence, reasoning}`
   - Possible verdicts: Accurate, Misleading, Biased, Inconclusive

2. **Structured Debate** (Turns 1-2)
   - Each agent reviews others' verdicts
   - Reconsiders position based on peer perspectives
   - Returns: `{verdict, confidence, reasoning, changed: true/false}`

3. **Consensus Calculation**
   - Count verdict distribution
   - Calculate consensus percentage: `(majority_count / 3) × 100`
   - Average confidence of majority agents

4. **Threshold Check**
   - **Pass**: ≥80% consensus + ≥70% avg confidence → Return verdict
   - **Fail**: <80% consensus → Return "Inconclusive"

**Key Design Choice**: High bar (80%) intentionally prevents unstable verdicts. "Inconclusive" is a feature, not a bug—it signals genuine ambiguity.

**File**: `src/consensus_protocol.py`

---

## Data Flow

### End-to-End Processing Pipeline

```
INPUT: article.txt (11,685 chars)
  ↓
[LAYER 1] Pillar Extraction
  → Gemini API call (60s timeout)
  → Returns: 5 pillars × 200 chars avg
  ↓
[LAYER 1] Stage 1 NER
  → spaCy en_core_web_md
  → Returns: Entity map (13 entities)
  ↓
[LAYER 1] Stage 2 NER (per pillar)
  → spaCy en_core_web_trf × 5 pillars
  → Returns: Enriched entities + temporal anchors
  ↓
[LAYER 2] Multi-Intent Search (per pillar)
  → Tavily API × 12 queries/pillar × 5 pillars = 60 API calls
  → Returns: 180 search results (3 results × 4 intents × 5 pillars × 3 queries)
  ↓
[LAYER 3] Embedding Generation
  → Gemini embedding API × 180 results
  → Returns: 180 × 768-dim vectors
  ↓
[LAYER 3] Qdrant Ingestion
  → Batch upsert (180 points)
  → Returns: Collection ready for search
  ↓
[LAYER 3] Conflict Detection (per pillar)
  → Qdrant query × 5 pillars
  → Returns: 3 conflicts detected
  ↓
[LAYER 4] Agent Debate
  → Initial verdicts: 3 agents × 1 LLM call = 3 calls
  → Debate turns: 2 rounds × 3 agents = 6 calls
  → Total: 9 Gemini API calls
  ↓
[LAYER 4] Consensus Calculation
  → Aggregate verdicts
  → Returns: Final verdict with confidence
  ↓
OUTPUT: audit_result.json (detailed analysis)
        Terminal summary (verdict + stats)
```

### API Call Summary (per article)

#### Full Mode
| API | Calls | Purpose |
|-----|-------|---------|
| Gemini (LLM) | 10 | 1 pillar extraction + 9 agent debate |
| Gemini (Embedding) | 180 | Search result vectorization |
| Tavily (Search) | 60 | Multi-intent search portfolio |
| **Total** | **250** | **Full analysis** |

**Estimated Runtime**: 60-90 seconds per article (network-bound)  
**Estimated Cost**: $0.48 per article (Tavily: 60 × $0.008)

#### Test Mode (`--test-mode`)
| API | Calls | Purpose |
|-----|-------|---------|
| Gemini (LLM) | 10 | 1 pillar extraction + 9 agent debate |
| Gemini (Embedding) | 6 | Search result vectorization |
| Tavily (Search) | 2 | Confirming + adversarial only |
| **Total** | **18** | **Concept validation** |

**Estimated Runtime**: 20-30 seconds per article (network-bound)  
**Estimated Cost**: $0.016 per article (Tavily: 2 × $0.008)  
**Free Tier**: 500 articles/month (vs 16-17 in full mode)

---

## Component Details

### File Structure

```
News-Audit-Engine/
├── cli.py                      # Command-line entry point
├── ARCHITECTURE.md             # This document
├── README.md                   # User guide
├── requirements.txt            # Python dependencies
├── .env                        # API keys (gitignored)
├── .env.example                # Template for .env
├── .gitignore                  # Excludes .env, __pycache__, qdrant_storage
│
├── src/                        # Core modules
│   ├── __init__.py             # Package marker
│   ├── gemini_client.py        # Gemini API wrapper (JSON + embeddings)
│   ├── utils.py                # Shared utilities (logging, formatting)
│   ├── ner_pipeline.py         # Layer 1: Two-stage NER
│   ├── search_portfolio.py     # Layer 2: Tavily multi-intent search
│   ├── semantic_judge.py       # Layer 3: Qdrant conflict detection
│   └── consensus_protocol.py   # Layer 4: Multi-agent debate
│
├── prompts/                    # LLM prompt configurations (JSON)
│   ├── pillar_extraction.json  # Narrative pillar prompt
│   └── agent_personas.json     # Agent role definitions
│
├── tests/                      # Unit tests (pytest)
│   └── test_ner_pipeline.py    # NER pipeline tests
│
└── [Runtime Outputs]
    ├── audit_result.json       # Detailed analysis results
    └── test_article.txt        # Sample input article
```

### Key Modules

#### `src/gemini_client.py`

**Class**: `GeminiClient`

**Methods**:
- `__init__(api_key: Optional[str])`: Initialize with API key
- `generate_json(prompt, schema, timeout, system_instruction, model, temperature)`:
  - Calls Gemini API with JSON response mode
  - Returns: `{ok: bool, data: dict, error: str, raw: dict}`
- `generate_embedding(text, model="text-embedding-004")`:
  - Generates 768-dim embeddings
  - Returns: `{ok: bool, embedding: list[float], dimensions: int, error: str}`

**Error Handling**: All API calls wrapped in try/except, return structured error dicts

---

#### `src/ner_pipeline.py`

**Class**: `NERPipeline`

**Methods**:
- `stage_one_contextual(text)`:
  - Fast NER pass with en_core_web_md
  - Returns: `{entities: list, entity_map: dict, entity_count: int}`
- `stage_two_precision(pillar_text, context_entities)`:
  - Precision NER with en_core_web_trf
  - Returns: `{entities: list, temporal_anchors: list, quotes: list}`
- `extract_pillars_with_entities(full_text, pillars)`:
  - Complete two-stage pipeline
  - Returns: List of enriched pillars

**Temporal Resolution**: Uses `dateparser` library to convert relative dates (e.g., "last Tuesday") to absolute ISO dates based on article publication date.

---

#### `src/search_portfolio.py`

**Class**: `SearchPortfolio`

**Methods**:
- `generate_search_queries(pillar)`:
  - Returns: `{confirming: list, adversarial: list, contextual: list, consensus: list}`
- `execute_search(query, search_type, max_results=5)`:
  - Single Tavily API call
  - Returns: List of `{title, url, content, score, search_type, query}`
- `execute_portfolio(pillar)`:
  - Runs all 4 search strategies for one pillar
  - Returns: `{pillar, searches: dict, total_results: int}`
- `search_all_pillars(pillars)`:
  - Batch execution across all pillars
  - Returns: List of portfolios

**Search Depth**: `advanced` mode in Tavily API for higher quality results

---

#### `src/semantic_judge.py`

**Class**: `SemanticJudge`

**Initialization**:
```python
judge = SemanticJudge(gemini_client, collection_name="audit_evidence")
```
- Creates in-memory Qdrant collection
- Configures: 768 dimensions, Cosine distance

**Methods**:
- `ingest_search_results(portfolios)`:
  - Embeds all search results
  - Batch upserts to Qdrant
  - Returns: Vector count
- `detect_conflicts(pillar_text, threshold=0.85)`:
  - Queries Qdrant for similar content
  - Groups by search intent
  - Returns: List of conflicts with source URLs
- `calculate_omission_score(article_pillars, consensus_facts)`:
  - Compares article to consensus baseline (TODO: implement fully)
  - Returns: `{omission_score: float, missing_facts: list}`

**Threshold**: 0.85 cosine similarity (85% match) required for conflict flagging

---

#### `src/consensus_protocol.py`

**Class**: `ConsensusProtocol`

**Agent Definitions** (from `prompts/agent_personas.json`):
```python
self.agents = {
    "auditor": {
        "name": "The Auditor",
        "role": "Logic & emotional framing",
        "prompt_template": "..."
    },
    "contextualist": {...},
    "skeptic": {...}
}
```

**Methods**:
- `gather_agent_verdicts(evidence)`:
  - Parallel LLM calls to all 3 agents
  - Returns: List of initial verdicts
- `conduct_debate(initial_verdicts, evidence, max_turns=2)`:
  - Each agent reviews others' positions
  - Runs 2 debate rounds
  - Returns: List of final verdicts
- `calculate_consensus(final_verdicts)`:
  - Aggregates verdicts
  - Checks 80% threshold
  - Returns: `{verdict, confidence, consensus_percentage, reasoning}`
- `run_full_protocol(evidence)`:
  - Orchestrates entire debate
  - Returns: `{consensus, initial_verdicts, final_verdicts, debate_turns}`

**Confidence Parsing**: Handles both float (0.7) and string ("70%" or "High") confidence values

---

#### `src/utils.py`

**Utility Functions**:
- `load_prompt(name, prompts_dir="prompts")`: Load JSON prompt configs
- `get_api_key(key_name)`: Get env var with error handling
- `truncate_text(text, max_chars=8000)`: Smart text truncation
- `format_entity(entity_text, entity_type, wikidata_id)`: Entity formatter

**Class**: `ProgressTracker`
- CLI progress logging with phase tracking
- Methods: `start_phase()`, `update()`, `complete_phase()`, `error()`

---

## Dependencies & Infrastructure

### Python Dependencies

**File**: `requirements.txt`

```
requests>=2.31.0           # HTTP client for API calls
python-dotenv>=1.0.0       # .env file loading
spacy>=3.7.0               # NER framework
dateparser>=1.2.0          # Temporal resolution
tavily-python>=0.3.0       # Search API
qdrant-client>=1.7.0       # Vector database
sayswho>=0.1.0             # Quote attribution (TODO)
numpy<2                    # Pinned for spaCy compatibility
```

### spaCy Models

```bash
# Medium model (50 MB) - Fast contextual NER
python -m spacy download en_core_web_md

# Transformer model (500 MB) - Precision NER
python -m spacy download en_core_web_trf
```

**Model Comparison**:

| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| en_core_web_md | 50 MB | Fast | Good | Stage 1: Full article |
| en_core_web_trf | 500 MB | Slow | Excellent | Stage 2: Pillars only |

### External APIs

**Gemini API** (Google)
- Endpoint: `https://generativelanguage.googleapis.com/v1beta/models`
- Models:
  - `gemini-2.5-flash-lite`: LLM (JSON mode)
  - `text-embedding-004`: Embeddings (768-dim)
- Rate Limit: ~60 req/min (free tier)
- Cost: Free tier sufficient for prototyping

**Tavily API**
- Endpoint: `https://api.tavily.com`
- Plan: Developer (free tier)
- Rate Limit: 1000 searches/month
- Features: Advanced search depth, content cleaning

**Qdrant**
- Deployment: In-memory mode (`:memory:`)
- No external service required for prototype
- Production: Docker container or Qdrant Cloud

---

## Prompts & LLM Integration

### Prompt Files

#### `prompts/pillar_extraction.json`

**Purpose**: Extract 3-5 narrative pillars from article

**Structure**:
```json
{
  "system_instruction": "You are an expert narrative analyst...",
  "user_template": "Analyze this article and extract 3-5 NARRATIVE PILLARS...",
  "timeout": 60
}
```

**Key Instructions**:
- "Each pillar should be a complete claim with subject, action, and consequence"
- "Represent a key causal link in the article's logic"
- "Be independently verifiable"
- "Not be mere background information"

**Output Schema**:
```json
{
  "pillars": [
    {
      "text": "The complete pillar statement",
      "importance": 5,
      "entities": ["Entity1", "Entity2"],
      "temporal_marker": "2025-01-15 or 'recent' or null"
    }
  ]
}
```

---

#### `prompts/agent_personas.json`

**Purpose**: Define agent roles and focus areas

**Structure**:
```json
{
  "auditor": {
    "name": "The Auditor",
    "role": "Logical consistency and emotional framing expert",
    "focus_areas": [
      "Internal logical consistency",
      "Emotional manipulation via loaded language",
      "Logical fallacies",
      "Misleading framing techniques"
    ]
  },
  "contextualist": {...},
  "skeptic": {...}
}
```

**Agent Prompt Templates** (embedded in `consensus_protocol.py`):

```python
auditor_template = """
You are The Auditor. Analyze the following evidence for logical consistency, 
emotional manipulation, and biased framing. Focus on:
- Are claims internally consistent?
- Is emotional language used to influence rather than inform?
- Are there logical fallacies or misleading framing?

Evidence:
{evidence}

Provide your assessment as JSON with keys: verdict, confidence, reasoning
"""
```

---

### LLM Configuration

**Temperature Settings**:
- Pillar extraction: 0.7 (creative yet structured)
- Initial verdicts: 0.7 (allows diverse perspectives)
- Debate reconsideration: 0.5 (more focused, less variation)

**Timeout Settings**:
- Pillar extraction: 60s
- Agent verdicts: 45s
- Embeddings: 30s

**JSON Mode**: All Gemini calls use `responseMimeType: "application/json"` to ensure structured output

---

## Configuration & Deployment

### Environment Variables

**File**: `.env` (create from `.env.example`)

```bash
GEMINI_API_KEY="your-gemini-api-key"
TAVILY_API_KEY="your-tavily-api-key"
```

**Security**: `.env` is gitignored. Never commit API keys.

---

### Installation Steps

```bash
# 1. Navigate to project
cd "/Users/jeff/Dropbox/UDEL/Code/News-Audit-Engine"

# 2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Download spaCy models
python -m spacy download en_core_web_md
python -m spacy download en_core_web_trf

# 5. Configure API keys
cp .env.example .env
# Edit .env and add your API keys

# 6. Verify installation
python cli.py "Test article text"
```

---

### Usage

**Analyze from file** (full mode):
```bash
python cli.py --file article.txt
```

**Analyze from file** (test mode - 97% cheaper):
```bash
python cli.py --file article.txt --test-mode
```

**Analyze from text**:
```bash
python cli.py "paste article text here"
```

**View results**:
```bash
cat audit_result.json | python -m json.tool | less
```

**Test Mode Benefits**:
- Analyzes only the highest importance pillar
- Uses only 2 Tavily queries (confirming + adversarial)
- Costs $0.016 per article (vs $0.48 in full mode)
- Enables 500 test articles on free tier (vs 16-17 in full mode)
- Validates entire pipeline architecture

---

## API Specifications

### CLI Interface

**Invocation**:
```bash
cli.py [--file <path>] [<text>] [--test-mode]
```

**Arguments**:
- `--file <path>`: Path to article text file
- `<text>`: Direct text input (alternative to --file)
- `--test-mode`: Enable test mode (1 pillar, 2 queries, 97% cost reduction)

**Output**:
- **Terminal**: Progress phases, final verdict summary
- **File**: `audit_result.json` (detailed analysis)

**Exit Codes**:
- `0`: Success
- `1`: Error (missing API keys, invalid input, API failure)

**Test Mode Behavior**:
- Extracts all narrative pillars (validates Layer 1)
- Selects highest importance pillar for analysis
- Runs only confirming and adversarial searches (skips contextual, consensus)
- Uses 1 query per intent type (vs 3 in full mode)
- Continues with full Layers 3 & 4 (semantic analysis + agent debate)
- **Result**: 2 Tavily API calls instead of 60

---

### JSON Output Schema

**File**: `audit_result.json`

```json
{
  "article_length": 11685,
  "pillars": [
    {
      "text": "Narrative pillar statement",
      "importance": 5,
      "entities": [
        {
          "text": "Entity name",
          "type": "PERSON | ORG | GPE",
          "start": 0,
          "end": 10,
          "context_match": true,
          "wikidata_id": "Q123" // Optional
        }
      ],
      "temporal_anchors": [
        {
          "original": "last Tuesday",
          "resolved": "2025-12-16",
          "start": 50,
          "end": 62
        }
      ],
      "quotes": [] // TODO: SaysWho integration
    }
  ],
  "search_results": 180,
  "conflicts": [
    {
      "pillar": "Pillar text snippet",
      "conflict_type": "opposing_evidence",
      "supporting_count": 5,
      "opposing_count": 6,
      "supporting_sources": ["url1", "url2", "url3"],
      "opposing_sources": ["url4", "url5", "url6"]
    }
  ],
  "consensus": {
    "consensus": {
      "verdict": "Inconsistent and Manipulative | Accurate | Misleading | Biased | Inconclusive",
      "confidence": 0.85,
      "consensus_percentage": 100.0,
      "agent_count": 3,
      "reasoning": "3/3 agents agreed"
    },
    "initial_verdicts": [
      {
        "verdict": "Inconsistent and Manipulative",
        "confidence": "High",
        "reasoning": "Detailed explanation...",
        "agent": "The Auditor",
        "agent_id": "auditor"
      }
    ],
    "final_verdicts": [
      {
        "verdict": "Inconsistent and Manipulative",
        "confidence": "High",
        "reasoning": "After debate...",
        "changed": false,
        "agent": "The Auditor",
        "agent_id": "auditor",
        "turn": 2
      }
    ],
    "debate_turns": 2
  }
}
```

---

## Future Enhancements

### Phase 0: Test Mode Enhancements (Completed ✅)

**Test Mode (`--test-mode`)**
- **Status**: Implemented December 25, 2025
- **Purpose**: Enable cost-effective testing and development
- **Behavior**: Analyzes 1 pillar with 2 queries (confirming + adversarial)
- **Savings**: 97% reduction in Tavily API costs ($0.48 → $0.016 per article)
- **Use Cases**:
  - Development and debugging
  - Architecture validation
  - Prompt engineering experiments
  - High-volume testing (500 articles on free tier)
- **Trade-offs**: Reduced evidence diversity, but core architecture fully validated

### Phase 1: Entity Enhancement (2-3 weeks)

**Wikidata Entity Linking**
- **Library**: `pywikibot` or Wikidata API
- **Goal**: Disambiguate entities (Apple Inc. vs. apple fruit)
- **Implementation**: Add `wikidata_id` to entity objects
- **Benefit**: Enables cross-article entity tracking

**SaysWho Quote Extraction**
- **Library**: `sayswho` package
- **Goal**: Extract quotes with attribution
- **Output**: `{quote, speaker, cue, start, end}`
- **Benefit**: Distinguish article claims from reported claims

---

### Phase 2: UI Integration (1-2 weeks)

**Option A: Standalone Web UI**
- Flask/FastAPI backend
- React frontend
- Real-time progress updates via WebSockets
- Interactive pillar visualization

**Option B: Project Truth Integration**
- Add "Narrative Audit" tab to existing Shiny app
- Call `audit_engine.analyze(content)` as module
- Display pillars, conflicts, agent debate in UI
- Keep existing Reality Taxonomy/Moral Foundations separate

**Recommendation**: Start with Option A for faster iteration, migrate to Option B once stable.

---

### Phase 3: Performance Optimization (1 week)

**Parallel API Calls**
- Use `asyncio` or `threading` for:
  - Embedding generation (180 calls → 10s parallelized)
  - Search execution (60 calls → 20s parallelized)
- Expected speedup: 60s → 30s per article

**Caching Layer**
- Cache embeddings for frequently searched content
- Redis or local SQLite
- Reduces redundant Gemini API calls

---

### Phase 4: Advanced Features (2-4 weeks)

**Omission Scoring Enhancement**
- Implement full vector similarity comparison
- Build consensus baseline from Reuters/AP archives
- Flag when article omits 50%+ of consensus facts

**Temporal Consistency Analysis**
- Cross-reference temporal claims against historical databases
- Flag anachronisms or timeline inconsistencies
- Use Wikidata for historical event dates

**Source Credibility Scoring**
- Integrate Media Bias/Fact Check API
- Weight search results by source reliability
- Adjust conflict detection thresholds based on source quality

**Multi-Language Support**
- Add spaCy models for other languages
- Translate pillars before search (Google Translate API)
- Requires language-specific Tavily searches

---

### Phase 5: Production Hardening (1-2 weeks)

**Persistent Qdrant**
- Docker container or Qdrant Cloud
- Enables cross-article analysis
- Build knowledge graph of recurring entities

**Error Recovery**
- Retry logic for transient API failures
- Checkpoint system for long-running analyses
- Graceful degradation (continue with partial results)

**Logging & Monitoring**
- Structured logging (JSON logs)
- Prometheus metrics
- Error alerting (Sentry)

**Testing**
- Unit tests for each module (pytest)
- Integration tests for full pipeline
- Regression tests with known articles

---

## Appendices

### A. Known Limitations

1. **Entity Linking**: Currently placeholder; Wikidata integration pending
2. **Quote Attribution**: SaysWho integration incomplete
3. **Omission Scoring**: Vector similarity comparison needs full implementation
4. **Temporal Resolution**: Requires publication date as reference; currently uses today's date
5. **Confidence Parsing**: String parsing heuristic (e.g., "High" → fallback) not robust
6. **Search Query Quality**: Basic template-based; could use LLM-generated queries
7. **Agent Debate Rounds**: Fixed at 2; could be adaptive based on consensus convergence
8. **Language Support**: English only

---

### B. Design Decisions Rationale

**Why Two-Stage NER?**
- Stage 1 (fast) provides context to prevent Stage 2 (slow) from misidentifying entities
- Running transformer model on full article (11k chars) takes 30s+; pillars only takes 5s
- Cost/benefit: 10x speedup for 5% accuracy loss

**Why Multi-Intent Search?**
- Single "confirming" search creates filter bubble
- Adversarial searches expose contradictory evidence
- Consensus searches establish authoritative baseline
- Contextual searches prevent missing forest for trees

**Why 80% Consensus Threshold?**
- Lower threshold (60%) allows "Misleading" verdict with 2/3 split—too unstable
- Higher threshold (100%) flags too many articles as "Inconclusive"
- 80% = 3/3 agreement, which empirically produces stable verdicts

**Why In-Memory Qdrant?**
- Prototype simplicity: no Docker setup required
- Fast iteration during development
- Easy migration to persistent storage later
- Per-article analysis doesn't need persistence (yet)

**Why Gemini 2.5 Flash Lite?**
- Fast (2-3s response time)
- Cheap (free tier sufficient)
- Strong JSON mode support
- 1M token context (sufficient for evidence summaries)

---

### C. Glossary

- **Narrative Pillar**: A causal argument that forms the logical backbone of a story (e.g., "Event A caused Event B because of Reason C")
- **Atomic Fact-Checking**: Traditional approach verifying individual facts in isolation (brittle, context-dependent)
- **Narrative Integrity**: Holistic evaluation of a story's logical coherence and evidence quality
- **Semantic Conflict**: Situation where highly similar content represents opposing viewpoints
- **Omission Score**: Percentage of consensus facts absent from an article
- **Consensus Threshold**: Minimum agent agreement required for a definitive verdict (80%)
- **Entity Linking**: Mapping entity mentions to unique identifiers (e.g., Wikidata IDs)
- **Temporal Anchoring**: Converting relative dates ("last Tuesday") to absolute dates ("2025-12-16")
- **Search Intent**: Purpose of a search query (confirming, adversarial, contextual, consensus)

---

### D. Performance Benchmarks

**Test Article**: 10,718 characters

#### Full Mode (5 pillars, 60 searches)
| Phase | Time | API Calls | Notes |
|-------|------|-----------|-------|
| Pillar Extraction | 8s | 1 Gemini | LLM processing time |
| Stage 1 NER | 2s | 0 | spaCy CPU-only |
| Stage 2 NER | 5s | 0 | spaCy transformer |
| Search Portfolio | 30s | 60 Tavily | Network-bound |
| Embedding Generation | 20s | 180 Gemini | Could parallelize |
| Qdrant Ingestion | 1s | 0 | In-memory write |
| Conflict Detection | 2s | 5 Qdrant | Vector search |
| Agent Debate | 25s | 9 Gemini | 3 initial + 6 debate |
| **Total** | **93s** | **250** | **~1.5 min per article** |
| **Cost** | **$0.48** | **Tavily only** | **~16 articles/month free** |

#### Test Mode (`--test-mode`: 1 pillar, 2 searches)
| Phase | Time | API Calls | Notes |
|-------|------|-----------|-------|
| Pillar Extraction | 8s | 1 Gemini | Same as full mode |
| Stage 1 NER | 2s | 0 | Same as full mode |
| Stage 2 NER | 1s | 0 | Only 1 pillar |
| Search Portfolio | 3s | 2 Tavily | 97% reduction |
| Embedding Generation | 2s | 6 Gemini | Only 6 results |
| Qdrant Ingestion | <1s | 0 | In-memory write |
| Conflict Detection | 1s | 1 Qdrant | Single pillar |
| Agent Debate | 25s | 9 Gemini | Same as full mode |
| **Total** | **42s** | **18** | **~40 sec per article** |
| **Cost** | **$0.016** | **Tavily only** | **~500 articles/month free** |

**Optimization Potential**: Parallelizing embeddings + search → ~20s total runtime in test mode

---

### E. Version History

| Version | Date | Changes |
|---------|------|---------|
| 4.0.1 | 2025-12-25 | Test mode implementation |
| | | - Added `--test-mode` flag for cost-effective testing |
| | | - 97% reduction in Tavily API costs ($0.48 → $0.016) |
| | | - Enables 500 test articles on free tier |
| | | - Validates full architecture with 1 pillar, 2 queries |
| 4.0.0 | 2025-12-24 | Initial architecture implementation |
| | | - Four-layer system operational |
| | | - CLI interface complete |
| | | - All core features working |
| | | - Known limitations documented |

---

### F. Contact & Contributing

**Project Status**: Private research prototype

**Future Plans**: 
- Complete Phase 1-2 enhancements
- Publish findings in academic paper
- Potential open-source release after validation

**Technical Debt**:
- TODO items marked in code comments
- Test coverage: ~10% (needs improvement)
- Documentation: Architecture complete, API docs pending

---

**Document Status**: Living document; updated as system evolves.

**Last Reviewed**: December 24, 2025
